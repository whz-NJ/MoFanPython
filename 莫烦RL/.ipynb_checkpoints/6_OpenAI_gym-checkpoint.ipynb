{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a21149",
   "metadata": {},
   "source": [
    "### 要点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1cdf8",
   "metadata": {},
   "source": [
    "手动编环境是一件很耗时间的事情, 所以如果有能力使用别人已经编好的环境, 可以节约我们很多时间. OpenAI gym 就是这样一个模块, 他提供了我们很多优秀的模拟环境. 我们的各种 RL 算法都能使用这些环境. 不过 OpenAI gym 暂时只支持 MacOS 和 Linux 系统. Windows 已经支持, 但是听说还没有全面支持, 大家时不时查看下官网, 可能就有惊喜. 是在等不及更新了, 也行用 tkinter 来手动编写一下环境. 这里有我制作的很好的 tkinter 入门教程, 之前的 maze 环境也是用 tkinter 编出来的. 大家可以仿照这个文件编写就 ok 啦. 或者还可以玩玩更厉害的, 想 OpenAI 一样, 使用 pyglet 模块来编写, 我做了一个从环境开始编写的强化学习实战."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf7a6e",
   "metadata": {},
   "source": [
    "### 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdcfb9",
   "metadata": {},
   "source": [
    "在 MacOS 和 Linux 系统下, 安装 gym 很方便, 首先确定你是 python 2.7 或者 python 3.5 版本. 然后在你的 terminal 中复制下面这些. 但是 gym 暂时还不完全支持 Windows, 不过有些虚拟环境已经的到了支持, 想立杆子那个已经支持了. 所以接下来要说的安装方法只有 MacOS 和 Linux 的. Windows 用户的安装方式应该也差不多, 如果 Windows 用户遇到了问题, 欢迎在留言区分享解决的方法.\n",
    "#python 2.7, 复制下面\n",
    "$ pip install gym\n",
    "\n",
    "#python 3.5, 复制下面\n",
    "$ pip3 install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9911776",
   "metadata": {},
   "source": [
    "如果没有报错, 恭喜你, 这样你就装好了 gym 的最基本款, 可以开始玩以下游戏啦:\n",
    "algorithmic\n",
    "toy_text\n",
    "classic_control (这个需要 pyglet 模块)\n",
    "如果在安装中遇到问题. 可能是缺少了一些必要模块, 可以使用下面语句来安装这些模块(安装时间可能有点久):\n",
    "#MacOS:\n",
    "\n",
    "$ brew install cmake boost boost-python sdl2 swig wget\n",
    "\n",
    "#Ubuntu 14.04:\n",
    "\n",
    "$ apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319b894",
   "metadata": {},
   "source": [
    "如果想要玩 gym 提供的全套游戏, 下面这几句就能满足你:\n",
    "\n",
    "#python 2.7, 复制下面\n",
    "\n",
    "$ pip install gym[all]\n",
    "\n",
    "#python 3.5, 复制下面\n",
    "\n",
    "$ pip3 install gym[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51cf6b",
   "metadata": {},
   "source": [
    "### CartPole 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d685342",
   "metadata": {},
   "source": [
    "之前我编写的 maze_env 基本上是按照 gym 环境格式写的, 所以你换成 gym 格式会很简单.\n",
    "\n",
    "接下来我们对应上面的算法, 来实现主循环. 首先 import 所需模块."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b202e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of code is the DQN brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "Using Tensorflow to build the neural network.\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DeepQNetwork:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=300,\n",
    "            memory_size=500,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],  # fixed params\n",
    "                self.s: batch_memory[:, :self.n_features],  # newest params\n",
    "            })\n",
    "\n",
    "        # change q_target w.r.t q_eval's action\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        For example in this batch I have 2 samples and 3 actions:\n",
    "        q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "        q_target = q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "        Then change q_target with the real q_target value w.r.t the q_eval's action.\n",
    "        For example in:\n",
    "            sample 0, I took action 0, and the max q_target value is -1;\n",
    "            sample 1, I took action 2, and the max q_target value is -2:\n",
    "        q_target =\n",
    "        [[-1, 2, 3],\n",
    "         [4, 5, -2]]\n",
    "        So the (q_target - q_eval) becomes:\n",
    "        [[(-1)-(1), 0, 0],\n",
    "         [0, 0, (-2)-(6)]]\n",
    "        We then backpropagate this error w.r.t the corresponding action to network,\n",
    "        leave other action as error=0 cause we didn't choose it.\n",
    "        \"\"\"\n",
    "\n",
    "        # train eval network\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "    def plot_cost(self):\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        %matplotlib inline\n",
    "        \n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd51f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Discrete(2)\n",
      "Box(4,)\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OS\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')   # 定义使用 gym 库中的那一个环境\n",
    "env = env.unwrapped # 不做这个会有很多限制\n",
    "\n",
    "print(env.action_space) # 查看这个环境中可用的 action 有多少个\n",
    "print(env.observation_space)    # 查看这个环境中可用的 state 的 observation 有多少个\n",
    "print(env.observation_space.high)   # 查看 observation 最高取值\n",
    "print(env.observation_space.low)    # 查看 observation 最低取值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a2294",
   "metadata": {},
   "source": [
    "于之前使用 tkinter 定义的环境有点不一样, 我们可以不适用 if __name__ == \"__main__\" 的方式, 下面是一种类似, 却更简单的写法. 之中我们会用到里面的 reward, 但是 env.step() 说提供的 reward 不一定是最有效率的 reward, 我们大可对这些进行修改, 使 DQN 学得更有效率. 你可以自己对比一下不修改 reward 和 按我这样修改, 他们学习过程的不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac579b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\OS\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "episode:  0 ep_r:  7.33  epsilon:  0\n",
      "episode:  1 ep_r:  3.13  epsilon:  0\n",
      "episode:  2 ep_r:  4.27  epsilon:  0\n",
      "episode:  3 ep_r:  11.24  epsilon:  0\n",
      "episode:  4 ep_r:  8.98  epsilon:  0\n",
      "episode:  5 ep_r:  2.58  epsilon:  0\n",
      "episode:  6 ep_r:  0.89  epsilon:  0\n",
      "episode:  7 ep_r:  4.67  epsilon:  0\n",
      "episode:  8 ep_r:  5.75  epsilon:  0\n",
      "episode:  9 ep_r:  6.36  epsilon:  0\n",
      "episode:  10 ep_r:  1.86  epsilon:  0\n",
      "episode:  11 ep_r:  6.04  epsilon:  0\n",
      "episode:  12 ep_r:  6.39  epsilon:  0\n",
      "episode:  13 ep_r:  1.91  epsilon:  0\n",
      "episode:  14 ep_r:  3.07  epsilon:  0\n",
      "episode:  15 ep_r:  16.51  epsilon:  0\n",
      "episode:  16 ep_r:  3.05  epsilon:  0\n",
      "episode:  17 ep_r:  2.33  epsilon:  0\n",
      "episode:  18 ep_r:  4.07  epsilon:  0\n",
      "episode:  19 ep_r:  3.82  epsilon:  0\n",
      "episode:  20 ep_r:  2.44  epsilon:  0\n",
      "episode:  21 ep_r:  3.2  epsilon:  0\n",
      "episode:  22 ep_r:  19.62  epsilon:  0\n",
      "episode:  23 ep_r:  8.41  epsilon:  0\n",
      "episode:  24 ep_r:  8.23  epsilon:  0\n",
      "episode:  25 ep_r:  10.82  epsilon:  0\n",
      "episode:  26 ep_r:  0.95  epsilon:  0\n",
      "episode:  27 ep_r:  9.24  epsilon:  0\n",
      "episode:  28 ep_r:  5.88  epsilon:  0\n",
      "episode:  29 ep_r:  1.56  epsilon:  0\n",
      "episode:  30 ep_r:  3.39  epsilon:  0\n",
      "episode:  31 ep_r:  2.89  epsilon:  0\n",
      "episode:  32 ep_r:  4.51  epsilon:  0\n",
      "episode:  33 ep_r:  6.25  epsilon:  0\n",
      "episode:  34 ep_r:  7.39  epsilon:  0\n",
      "episode:  35 ep_r:  9.14  epsilon:  0\n",
      "episode:  36 ep_r:  4.14  epsilon:  0\n",
      "episode:  37 ep_r:  22.78  epsilon:  0\n",
      "episode:  38 ep_r:  0.96  epsilon:  0\n",
      "episode:  39 ep_r:  4.81  epsilon:  0\n",
      "episode:  40 ep_r:  17.7  epsilon:  0\n",
      "episode:  41 ep_r:  7.36  epsilon:  0\n",
      "episode:  42 ep_r:  3.69  epsilon:  0\n",
      "episode:  43 ep_r:  1.61  epsilon:  0\n",
      "episode:  44 ep_r:  4.79  epsilon:  0\n",
      "episode:  45 ep_r:  4.04  epsilon:  0\n",
      "episode:  46 ep_r:  3.8  epsilon:  0\n",
      "episode:  47 ep_r:  3.73  epsilon:  0\n",
      "episode:  48 ep_r:  2.69  epsilon:  0\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  49 ep_r:  10.72  epsilon:  0.0\n",
      "episode:  50 ep_r:  4.78  epsilon:  0.02\n",
      "episode:  51 ep_r:  5.2  epsilon:  0.04\n",
      "episode:  52 ep_r:  5.83  epsilon:  0.06\n",
      "episode:  53 ep_r:  4.83  epsilon:  0.07\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  54 ep_r:  2.86  epsilon:  0.08\n",
      "episode:  55 ep_r:  16.13  epsilon:  0.11\n",
      "episode:  56 ep_r:  3.93  epsilon:  0.13\n",
      "episode:  57 ep_r:  6.76  epsilon:  0.15\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  58 ep_r:  5.73  epsilon:  0.16\n",
      "episode:  59 ep_r:  7.81  epsilon:  0.18\n",
      "episode:  60 ep_r:  3.36  epsilon:  0.19\n",
      "episode:  61 ep_r:  4.94  epsilon:  0.21\n",
      "episode:  62 ep_r:  4.28  epsilon:  0.22\n",
      "episode:  63 ep_r:  5.25  epsilon:  0.24\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  64 ep_r:  7.26  epsilon:  0.25\n",
      "episode:  65 ep_r:  1.59  epsilon:  0.26\n",
      "episode:  66 ep_r:  4.96  epsilon:  0.3\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  67 ep_r:  15.1  epsilon:  0.33\n",
      "episode:  68 ep_r:  6.86  epsilon:  0.35\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  69 ep_r:  63.97  epsilon:  0.52\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  70 ep_r:  19.35  epsilon:  0.61\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  71 ep_r:  25.4  epsilon:  0.69\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  72 ep_r:  61.85  epsilon:  0.83\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  73 ep_r:  32.18  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  74 ep_r:  25.04  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  75 ep_r:  82.9  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  76 ep_r:  15.91  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  77 ep_r:  81.67  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  78 ep_r:  33.87  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  79 ep_r:  51.24  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  80 ep_r:  59.42  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  81 ep_r:  208.41  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  82 ep_r:  73.04  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  83 ep_r:  80.61  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  84 ep_r:  118.45  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  85 ep_r:  74.1  epsilon:  0.9\n",
      "episode:  86 ep_r:  8.22  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  87 ep_r:  45.34  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  88 ep_r:  51.48  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  89 ep_r:  56.64  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  90 ep_r:  78.49  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  91 ep_r:  86.64  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  92 ep_r:  109.86  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  93 ep_r:  220.71  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  94 ep_r:  253.87  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  95 ep_r:  360.66  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  96 ep_r:  779.33  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  97 ep_r:  62.55  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  98 ep_r:  210.27  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  99 ep_r:  334.71  epsilon:  0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 定义使用 DQN 的算法\n",
    "RL = DeepQNetwork(n_actions=env.action_space.n,\n",
    "                  n_features=env.observation_space.shape[0],\n",
    "                  learning_rate=0.01, e_greedy=0.9,\n",
    "                  replace_target_iter=100, memory_size=2000,\n",
    "                  e_greedy_increment=0.0008,)\n",
    "\n",
    "total_steps = 0 # 记录步数\n",
    "\n",
    "for i_episode in range(50):\n",
    "\n",
    "    # 获取回合 i_episode 第一个 observation\n",
    "    observation = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()    # 刷新环境\n",
    "\n",
    "        action = RL.choose_action(observation)  # 选行为\n",
    "\n",
    "        observation_, reward, done, info = env.step(action) # 获取下一个 state\n",
    "\n",
    "        x, x_dot, theta, theta_dot = observation_   # 细分开, 为了修改原配的 reward\n",
    "\n",
    "        # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少\n",
    "        # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高\n",
    "\n",
    "        x, x_dot, theta, theta_dot = observation_\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2   # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率\n",
    "\n",
    "        # 保存这一组记忆\n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        if total_steps > 1000:\n",
    "            RL.learn()  # 学习\n",
    "\n",
    "        ep_r += reward\n",
    "        if done:\n",
    "            print('episode: ', i_episode,\n",
    "                  'ep_r: ', round(ep_r, 2),\n",
    "                  ' epsilon: ', round(RL.epsilon, 2))\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n",
    "# 最后输出 cost 曲线\n",
    "RL.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1ec6c",
   "metadata": {},
   "source": [
    "### MountainCar 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddaffc",
   "metadata": {},
   "source": [
    "代码基本和上述代码相同, 就只是在 reward 上动了下手脚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0775e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Discrete(3)\n",
      "Box(2,)\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "WARNING:tensorflow:From C:\\Users\\OS\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OS\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  0 | Get | Ep_r:  2250.5541 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  1 | Get | Ep_r:  134.8135 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  2 | Get | Ep_r:  82.6953 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  3 | Get | Ep_r:  113.3331 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  4 | Get | Ep_r:  60.336 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  5 | Get | Ep_r:  69.7489 | Epsilon:  0.9\n",
      "Epi:  6 | Get | Ep_r:  74.4834 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  7 | Get | Ep_r:  67.2401 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  8 | Get | Ep_r:  71.3012 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  9 | Get | Ep_r:  40.6306 | Epsilon:  0.9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "RL = DeepQNetwork(n_actions=3, n_features=2, learning_rate=0.001, e_greedy=0.9,\n",
    "                  replace_target_iter=300, memory_size=3000,\n",
    "                  e_greedy_increment=0.0001,)\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "for i_episode in range(10):\n",
    "\n",
    "    observation = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        action = RL.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "\n",
    "        position, velocity = observation_\n",
    "\n",
    "        # 车开得越高 reward 越大\n",
    "        reward = abs(position - (-0.5))\n",
    "\n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        if total_steps > 1000:\n",
    "            RL.learn()\n",
    "\n",
    "        ep_r += reward\n",
    "        if done:\n",
    "            get = '| Get' if observation_[0] >= env.unwrapped.goal_position else '| ----'\n",
    "            print('Epi: ', i_episode,\n",
    "                  get,\n",
    "                  '| Ep_r: ', round(ep_r, 4),\n",
    "                  '| Epsilon: ', round(RL.epsilon, 2))\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n",
    "\n",
    "RL.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99cd38",
   "metadata": {},
   "source": [
    "这两个都只是例子而已, 具体的实施你也可以大动手脚, 比如你的 reward 定义得更好, 你的神经网络结构更好, 使得他们学的更快. 这些都是自己定义的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bafc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
