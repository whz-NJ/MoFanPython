{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd00be04",
   "metadata": {},
   "source": [
    "学习资料:\n",
    "\n",
    "+ [相关代码](https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tensorflow11_build_network.py)\n",
    "+ [为 TF 2017 打造的新版可视化教学代码](https://github.com/MorvanZhou/Tensorflow-Tutorial)\n",
    "\n",
    "\n",
    "这次提到了怎样建造一个完整的神经网络,包括添加神经层,计算误差,训练步骤,判断是否在学习.\n",
    "\n",
    "本次课程，我们会在上节课的基础上，继续讲解如何构建神经层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317de3e",
   "metadata": {},
   "source": [
    "### add_layer 功能\n",
    "首先，我们导入本次所需的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05dc62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ab8e9",
   "metadata": {},
   "source": [
    "构造添加一个神经层的函数。（在上次课程中有详细介绍）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674b38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size])) #网络内部用Variable\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) #网络内部用Variable\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f48af3",
   "metadata": {},
   "source": [
    "### 导入数据\n",
    "构建所需的数据。 这里的x_data和y_data并不是严格的一元二次函数的关系，因为我们多加了一个noise,这样看起来会更像真实情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f30e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)\n",
    "y_data = np.square(x_data) - 0.5 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c7f62",
   "metadata": {},
   "source": [
    "利用占位符定义我们所需的神经网络的输入。 tf.placeholder()就是代表占位符，这里的None代表无论输入有多少都可以，因为输入只有一个特征，所以这里是1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09867ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = tf.placeholder(tf.float32, [None, 1])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98be1ad",
   "metadata": {},
   "source": [
    "接下来，我们就可以开始定义神经层了。 通常神经层都包括输入层、隐藏层和输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有10个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 所以，我们构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5f592",
   "metadata": {},
   "source": [
    "### 搭建网络\n",
    "下面，我们开始定义隐藏层,利用之前的add_layer()函数，这里使用 Tensorflow 自带的激励函数tf.nn.relu。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46772d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093abcd1",
   "metadata": {},
   "source": [
    "接着，定义输出层。此时的输入就是隐藏层的输出——l1，输入有10层（隐藏层的输出层），输出有1层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec1d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = add_layer(l1, 10, 1, activation_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b1f17",
   "metadata": {},
   "source": [
    "计算预测值prediction和真实值的误差，对二者差的平方求和再取平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3eadc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                     reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f6967",
   "metadata": {},
   "source": [
    "接下来，是很关键的一步，如何让机器学习提升它的准确率。tf.train.GradientDescentOptimizer()中的值通常都小于1，这里取的是0.1，代表以0.1的效率来最小化误差loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4cb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d0c81",
   "metadata": {},
   "source": [
    "使用变量时，都要对它进行初始化，这是必不可少的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939c2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法\n",
    "init = tf.global_variables_initializer()  # 替换成这样就好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e14db6",
   "metadata": {},
   "source": [
    "定义Session，并用 Session 来执行 init 初始化步骤。 （注意：在tensorflow中，只有session.run()才会执行我们定义的运算。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0cad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2991b",
   "metadata": {},
   "source": [
    "### 训练\n",
    "下面，让机器开始学习。\n",
    "\n",
    "比如这里，我们让机器学习1000次。机器学习的内容是train_step, 用 Session 来 run 每一次 training 的数据，逐步提升神经网络的预测准确性。 (注意：**当运算要用到placeholder时，就需要feed_dict这个字典来指定输入**。)\n",
    "\n",
    "\n",
    "每50步我们输出一下机器学习的误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc4c7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002893726\n",
      "0.0028762731\n",
      "0.0028623426\n",
      "0.0028491507\n",
      "0.002834899\n",
      "0.0028226406\n",
      "0.00281094\n",
      "0.0028005396\n",
      "0.0027911868\n",
      "0.0027823248\n",
      "0.0027748342\n",
      "0.0027685023\n",
      "0.002762245\n",
      "0.0027565367\n",
      "0.0027518233\n",
      "0.0027477404\n",
      "0.0027440395\n",
      "0.002740526\n",
      "0.0027373005\n",
      "0.002734334\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    # training\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 50 == 0:\n",
    "        # to see the step improvement\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "上面是在电脑上运行本次代码的结果。通过上图可以看出，误差在逐渐减小，这说明机器学习是有积极的效果的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
